{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n",
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "import torch\n",
    "from vggt.models.vggt import VGGT\n",
    "from vggt.utils.load_fn import load_and_preprocess_images\n",
    "from vggt.utils.pose_enc import pose_encoding_to_extri_intri\n",
    "from vggt.utils.geometry import unproject_depth_map_to_point_map\n",
    "\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "import cv2\n",
    "from tqdm import tqdm,trange\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# bfloat16 is supported on Ampere GPUs (Compute Capability 8.0+) \n",
    "dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13777/2406001790.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(_URL))\n"
     ]
    }
   ],
   "source": [
    "model = VGGT()\n",
    "_URL = \"./model.pt\"\n",
    "model.load_state_dict(torch.load(_URL))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 不固定frontal view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(range(1, 21))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4550/1728077492.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=dtype):\n",
      "100%|██████████| 20/20 [14:31:54<00:00, 2615.71s/it]  \n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm(idxs):\n",
    "    idx = str(idx)\n",
    "\n",
    "    base_path = \"/root/autodl-tmp/facescape\"\n",
    "    img_path = os.path.join(base_path, \"mv_image\", idx)\n",
    "    depth_path = os.path.join(base_path, \"depth\", idx)\n",
    "    mask_path = os.path.join(base_path, \"mask\", idx)\n",
    "    params_path = os.path.join(base_path, \"params\", idx)\n",
    "\n",
    "    n_imgs = len(os.listdir(img_path))\n",
    "    def load_data(load_indices):\n",
    "        depth_map = []\n",
    "        extrinsic = []\n",
    "        intrinsic = []\n",
    "        images = []\n",
    "        masks = []\n",
    "\n",
    "        for load_index in load_indices:\n",
    "            if False:\n",
    "                img, depth, mask, extr, intr = data_cache[load_index]\n",
    "            else:\n",
    "                img = cv2.imread(os.path.join(img_path, f\"{load_index}.png\"))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                # img = cv2.resize(img, (518, round(2592/1728*518)))\n",
    "                # img = img[130:130+518, 0:518]\n",
    "                depth = np.load(os.path.join(depth_path, f\"{load_index}.npy\"))\n",
    "                # depth = cv2.resize(depth, (518, round(2592/1728*518)))\n",
    "                # depth = depth[130:130+518, 0:518]\n",
    "                mask = np.load(os.path.join(mask_path, f\"{load_index}.npy\"))\n",
    "                # mask = cv2.resize(mask, (518, round(2592/1728*518)))\n",
    "                # mask = mask[130:130+518, 0:518]\n",
    "                params = np.load(os.path.join(params_path, f\"{load_index}.npz\"))\n",
    "                extr = params[\"extr\"]\n",
    "                intr = params[\"intr\"]\n",
    "                # intr = intr * 518 / 1728\n",
    "                # intr[1,2] = intr[1,2] - 130\n",
    "                # data_cache[load_index] = (img, depth, mask, extr, intr)\n",
    "            depth_map.append(depth)\n",
    "            extrinsic.append(extr)\n",
    "            intrinsic.append(intr)\n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "\n",
    "        images = np.array(images)\n",
    "        images = images / 255.\n",
    "        masks = np.array(masks)\n",
    "        depth_map = np.array(depth_map)\n",
    "        extrinsic = np.array(extrinsic)\n",
    "        intrinsic = np.array(intrinsic)\n",
    "        return images, depth_map, extrinsic, intrinsic, masks\n",
    "\n",
    "    all_images, all_depth_maps, all_extrinsics, all_intrinsics, all_masks = load_data(list(range(n_imgs)))\n",
    "\n",
    "    def load_data(load_indices):\n",
    "        load_indices = list(load_indices)\n",
    "        images = all_images[load_indices]\n",
    "        depth_maps = all_depth_maps[load_indices]\n",
    "        extrinsics = all_extrinsics[load_indices]\n",
    "        intrinsics = all_intrinsics[load_indices]\n",
    "        masks = all_masks[load_indices]\n",
    "        return images, depth_maps, extrinsics, intrinsics, masks\n",
    "\n",
    "    load_indices_samples = set()\n",
    "    while True:\n",
    "        samples = list(range(n_imgs))\n",
    "        random.shuffle(samples)\n",
    "        samples = samples[:16]\n",
    "        samples = sorted(samples)\n",
    "        load_indices_samples.add(tuple(samples))\n",
    "        if len(load_indices_samples) == 1000:\n",
    "            break\n",
    "    load_indices_samples = list(load_indices_samples)\n",
    "    train_indices = load_indices_samples[:800]\n",
    "    val_indices = load_indices_samples[800:900]\n",
    "    test_indices = load_indices_samples[900:]\n",
    "    \n",
    "    feature_maps_avg = np.zeros((n_imgs, 518, 518, 128), dtype=np.float32)\n",
    "    n_feature_maps = np.zeros((n_imgs), dtype=np.float32)\n",
    "\n",
    "    for i in range(400):\n",
    "        load_indices = train_indices[i]\n",
    "        images, depth_map, extrinsic, intrinsic, masks = load_data(load_indices)\n",
    "        images = torch.from_numpy(images).to(device).float()\n",
    "        depth_map = torch.from_numpy(depth_map).to(device).unsqueeze(-1)\n",
    "        extrinsic = torch.from_numpy(extrinsic).to(device)\n",
    "        intrinsic = torch.from_numpy(intrinsic).to(device)\n",
    "        images = images.permute(0, 3, 1, 2) # 16, 518, 518, 3 -> 16, 3, 518, 518\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                images = images[None]  # add batch dimension\n",
    "                aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "            feature_maps = model.track_head.feature_extractor(aggregated_tokens_list, images, ps_idx)[0]\n",
    "            del images\n",
    "            feature_maps = torch.nn.functional.interpolate(feature_maps, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "            feature_maps = feature_maps.permute(0, 2, 3, 1) # 16, 518, 518, 128\n",
    "            feature_maps_ = feature_maps\n",
    "        \n",
    "        for j in range(len(load_indices)):\n",
    "            index = load_indices[j]\n",
    "            feature_maps_avg[index] = feature_maps_avg[index] * n_feature_maps[index] / (n_feature_maps[index]+1) + feature_maps_[j].detach().cpu().numpy()/ (n_feature_maps[index]+1)\n",
    "            n_feature_maps[index] += 1\n",
    "    all_points = unproject_depth_map_to_point_map(torch.from_numpy(all_depth_maps).to(device).unsqueeze(-1),\n",
    "                                    torch.from_numpy(all_extrinsics).to(device), \n",
    "                                    torch.from_numpy(all_intrinsics).to(device)) # 16, 518, 518, 3\n",
    "    all_points = all_points * 4\n",
    "    all_points = all_points[all_masks > 0] # K,3\n",
    "    all_features = feature_maps_avg[all_masks > 0] # K,128\n",
    "    os.makedirs(f\"/root/autodl-tmp/facescape/all_features/{idx}\", exist_ok=True)\n",
    "    np.save(f\"/root/autodl-tmp/facescape/all_features/{idx}/all_points.npy\", all_points)\n",
    "    np.save(f\"/root/autodl-tmp/facescape/all_features/{idx}/all_features.npy\", all_features)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3073/1946974649.py:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=dtype):\n",
      "  6%|▋         | 51/800 [05:04<1:13:30,  5.89s/it]"
     ]
    }
   ],
   "source": [
    "# feature_maps_avg = np.zeros((n_imgs, 518, 518, 128), dtype=np.float32)\n",
    "# n_feature_maps = np.zeros((n_imgs), dtype=np.float32)\n",
    "\n",
    "\n",
    "# batch_size = 1\n",
    "# for i in trange(0,800,batch_size):\n",
    "#     load_indices_this_batch = train_indices[i:i+batch_size]\n",
    "#     images_batch = []\n",
    "#     for item_ind, load_indices in enumerate(load_indices_this_batch):\n",
    "#         images, depth_map, extrinsic, intrinsic, masks = load_data(load_indices)\n",
    "#         images = torch.from_numpy(images).to(device).float()\n",
    "#         depth_map = torch.from_numpy(depth_map).to(device).unsqueeze(-1)\n",
    "#         extrinsic = torch.from_numpy(extrinsic).to(device)\n",
    "#         intrinsic = torch.from_numpy(intrinsic).to(device)\n",
    "#         images = images.permute(0, 3, 1, 2) # 16, 518, 518, 3 -> 16, 3, 518, 518\n",
    "#         images_batch.append(images)\n",
    "#     images = torch.stack(images_batch, dim=0)  # batch_size, 16, 3, 518, 518\n",
    "#     # change images to continuous memory layout\n",
    "#     # images = images.contiguous()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         with torch.cuda.amp.autocast(dtype=dtype):\n",
    "#             aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "#         feature_maps = model.track_head.feature_extractor(aggregated_tokens_list, images, ps_idx) # batch_size, 16, 128, 259, 259\n",
    "#         del images\n",
    "#         # convert feature_maps to batch_size*16, 128, 518, 518\n",
    "#         feature_maps = feature_maps.view(-1, feature_maps.shape[2], feature_maps.shape[3], feature_maps.shape[4])  # batch_size*16, 128, 259, 259\n",
    "#         feature_maps = torch.nn.functional.interpolate(feature_maps, size=(518, 518), mode='bilinear', align_corners=False) # batch_size * 16, 128, 518, 518\n",
    "#         feature_maps = feature_maps.view(len(load_indices_this_batch), 16, 128, 518, 518)\n",
    "#         feature_maps = feature_maps.permute(0, 1, 3, 4, 2)\n",
    "#         feature_maps_batch = feature_maps\n",
    "        \n",
    "#     for item_ind, load_indices in enumerate(load_indices_this_batch):\n",
    "#         feature_maps_ = feature_maps_batch[item_ind]\n",
    "#         for j in range(len(load_indices)):\n",
    "#             index = load_indices[j]\n",
    "#             feature_maps_avg[index] = feature_maps_avg[index] * n_feature_maps[index] / (n_feature_maps[index]+1) + feature_maps_[j].detach().cpu().numpy()/ (n_feature_maps[index]+1)\n",
    "#             n_feature_maps[index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./test_model/shadow1_16view\"\n",
    "\n",
    "image_names = sorted(glob.glob(os.path.join(path, \"*.png\")))\n",
    "n_imgs = len(image_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/tmp/ipykernel_21580/1049740084.py:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=dtype):\n",
      "100%|██████████| 100/100 [12:40<00:00,  7.60s/it]\n"
     ]
    }
   ],
   "source": [
    "feature_maps_avg = np.zeros((n_imgs, 518, 518, 128), dtype=np.float32)\n",
    "n_feature_maps = np.zeros((n_imgs), dtype=np.float32)\n",
    "for i in trange(100):\n",
    "    load_image_names = random.sample(image_names, 16)\n",
    "    images = load_and_preprocess_images(load_image_names).to(device)\n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast(dtype=dtype):\n",
    "            images = images[None]  # add batch dimension\n",
    "            aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "        feature_maps = model.track_head.feature_extractor(aggregated_tokens_list, images, ps_idx)[0]\n",
    "        feature_maps = torch.nn.functional.interpolate(feature_maps, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "        feature_maps = feature_maps.permute(0, 2, 3, 1) # 16, 518, 518, 128\n",
    "        feature_maps_ = feature_maps.detach().cpu().numpy()\n",
    "    for j in range(len(image_names)):\n",
    "        load_image_name = load_image_names[j]\n",
    "        index = image_names.index(load_image_name)\n",
    "        feature_maps_avg[index] = feature_maps_avg[index] * n_feature_maps[index] / (n_feature_maps[index]+1) + feature_maps_[j]/ (n_feature_maps[index]+1)\n",
    "        n_feature_maps[index] += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./feature_maps.npy\", feature_maps_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固定frontal view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontal_view_img_idx_dict = {\n",
    "    \"1\":0,\n",
    "    \"3\":15,\n",
    "    \"4\":0,\n",
    "    \"5\":14,\n",
    "    \"6\":15,\n",
    "    \"7\":15,\n",
    "    \"8\":30,\n",
    "    \"9\":15,\n",
    "    \"10\":0,\n",
    "    \"11\":14,\n",
    "    \"12\":22,\n",
    "    \"13\":45,\n",
    "    \"14\":18,\n",
    "    \"15\":34,\n",
    "    \"16\":0,\n",
    "    \"17\":14,\n",
    "    \"18\":0,\n",
    "    \"19\":0,\n",
    "    \"20\":33,\n",
    "    \"21\":15,\n",
    "    \"22\":0,\n",
    "    \"23\":0,\n",
    "    \"24\":0,\n",
    "    \"25\":18,\n",
    "    \"26\":0,\n",
    "    \"27\":0,\n",
    "    \"28\":18,\n",
    "    \"29\":15,\n",
    "    \"30\":14,\n",
    "    \"31\":43,\n",
    "    \"32\":14,\n",
    "    \"33\":15,\n",
    "    \"34\":15,\n",
    "    \"35\":0,\n",
    "    \"36\":0,\n",
    "    \"37\":15,\n",
    "    \"38\":0,\n",
    "    \"39\":17,\n",
    "    \"40\":18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontal_view_img_idx_dict = {\n",
    "    \"41\":37,\n",
    "    \"42\":34,\n",
    "    \"43\":24,\n",
    "    \"44\":14,\n",
    "    \"45\":15,\n",
    "    \"46\":0,\n",
    "    \"47\":0,\n",
    "    \"48\":0,\n",
    "    \"49\":18,\n",
    "    \"50\":18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2354/1820300978.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=dtype):\n",
      "100%|██████████| 10/10 [1:53:03<00:00, 678.33s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx in tqdm(frontal_view_img_idx_dict.keys()):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    frontal_view_img_idx = frontal_view_img_idx_dict[idx]\n",
    "    base_path = \"/root/autodl-tmp/facescape\"\n",
    "    img_path = os.path.join(base_path, \"mv_image\", idx)\n",
    "    depth_path = os.path.join(base_path, \"depth\", idx)\n",
    "    mask_path = os.path.join(base_path, \"mask\", idx)\n",
    "    params_path = os.path.join(base_path, \"params\", idx)\n",
    "\n",
    "    n_imgs = len(os.listdir(img_path))\n",
    "    def load_data(load_indices):\n",
    "        depth_map = []\n",
    "        extrinsic = []\n",
    "        intrinsic = []\n",
    "        images = []\n",
    "        masks = []\n",
    "\n",
    "        for load_index in load_indices:\n",
    "            if False:\n",
    "                img, depth, mask, extr, intr = data_cache[load_index]\n",
    "            else:\n",
    "                img = cv2.imread(os.path.join(img_path, f\"{load_index}.png\"))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                # img = cv2.resize(img, (518, round(2592/1728*518)))\n",
    "                # img = img[130:130+518, 0:518]\n",
    "                depth = np.load(os.path.join(depth_path, f\"{load_index}.npy\"))\n",
    "                # depth = cv2.resize(depth, (518, round(2592/1728*518)))\n",
    "                # depth = depth[130:130+518, 0:518]\n",
    "                mask = np.load(os.path.join(mask_path, f\"{load_index}.npy\"))\n",
    "                # mask = cv2.resize(mask, (518, round(2592/1728*518)))\n",
    "                # mask = mask[130:130+518, 0:518]\n",
    "                params = np.load(os.path.join(params_path, f\"{load_index}.npz\"))\n",
    "                extr = params[\"extr\"]\n",
    "                intr = params[\"intr\"]\n",
    "                # intr = intr * 518 / 1728\n",
    "                # intr[1,2] = intr[1,2] - 130\n",
    "                # data_cache[load_index] = (img, depth, mask, extr, intr)\n",
    "            depth_map.append(depth)\n",
    "            extrinsic.append(extr)\n",
    "            intrinsic.append(intr)\n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "\n",
    "        images = np.array(images)\n",
    "        images = images / 255.\n",
    "        masks = np.array(masks)\n",
    "        depth_map = np.array(depth_map)\n",
    "        extrinsic = np.array(extrinsic)\n",
    "        intrinsic = np.array(intrinsic)\n",
    "        return images, depth_map, extrinsic, intrinsic, masks\n",
    "\n",
    "    all_images, all_depth_maps, all_extrinsics, all_intrinsics, all_masks = load_data(list(range(n_imgs)))\n",
    "\n",
    "    def load_data(load_indices):\n",
    "        load_indices = list(load_indices)\n",
    "        images = all_images[load_indices]\n",
    "        depth_maps = all_depth_maps[load_indices]\n",
    "        extrinsics = all_extrinsics[load_indices]\n",
    "        intrinsics = all_intrinsics[load_indices]\n",
    "        masks = all_masks[load_indices]\n",
    "        return images, depth_maps, extrinsics, intrinsics, masks\n",
    "\n",
    "    load_indices_samples = set()\n",
    "    while True:\n",
    "        samples = list(range(n_imgs))\n",
    "        random.shuffle(samples)\n",
    "        samples = samples[:16]\n",
    "        samples = sorted(samples)\n",
    "        if frontal_view_img_idx in samples:\n",
    "            # make sure the frontal view is the first image\n",
    "            samples.remove(frontal_view_img_idx)\n",
    "            samples.insert(0, frontal_view_img_idx)\n",
    "        else:\n",
    "            continue\n",
    "        load_indices_samples.add(tuple(samples))\n",
    "        if len(load_indices_samples) == 1000:\n",
    "            break\n",
    "    load_indices_samples = list(load_indices_samples)\n",
    "    train_indices = load_indices_samples[:800]\n",
    "    val_indices = load_indices_samples[800:900]\n",
    "    test_indices = load_indices_samples[900:]\n",
    "\n",
    "    feature_maps_sum = np.zeros((n_imgs, 518, 518, 128), dtype=np.float32)\n",
    "    n_feature_maps = np.zeros((n_imgs), dtype=np.float32)\n",
    "    feature_maps_sum = torch.zeros((n_imgs, 518, 518, 128), dtype=torch.float32).to(device)\n",
    "    n_feature_maps = torch.zeros((n_imgs), dtype=torch.float32).to(device)\n",
    "\n",
    "    for i in range(400):\n",
    "        load_indices = train_indices[i]\n",
    "        load_indices = list(load_indices)\n",
    "        images, depth_map, extrinsic, intrinsic, masks = load_data(load_indices)\n",
    "        images = torch.from_numpy(images).to(device).float()\n",
    "        # depth_map = torch.from_numpy(depth_map).to(device).unsqueeze(-1)\n",
    "        # extrinsic = torch.from_numpy(extrinsic).to(device)\n",
    "        # intrinsic = torch.from_numpy(intrinsic).to(device)\n",
    "        images = images.permute(0, 3, 1, 2) # 16, 518, 518, 3 -> 16, 3, 518, 518\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                images = images[None]  # add batch dimension\n",
    "                aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "            feature_maps = model.track_head.feature_extractor(aggregated_tokens_list, images, ps_idx)[0]\n",
    "            del images\n",
    "            del aggregated_tokens_list\n",
    "            feature_maps = torch.nn.functional.interpolate(feature_maps, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "            feature_maps = feature_maps.permute(0, 2, 3, 1) # 16, 518, 518, 128\n",
    "            feature_maps_ = feature_maps\n",
    "            # feature_maps_ = feature_maps_.detach().cpu().numpy()  # convert to numpy array for further processing\n",
    "\n",
    "            # for j in range(len(load_indices)):\n",
    "            #     index = load_indices[j]\n",
    "            #     feature_maps_avg[index] = feature_maps_avg[index] * n_feature_maps[index] / (n_feature_maps[index]+1) + feature_maps_[j]/ (n_feature_maps[index]+1)\n",
    "            #     n_feature_maps[index] += 1\n",
    "            # feature_maps_avg[load_indices] = feature_maps_avg[load_indices] * n_feature_maps[load_indices][:, None, None, None] / (n_feature_maps[load_indices][:, None, None, None]+1) + feature_maps_ / (n_feature_maps[load_indices][:, None, None, None]+1)\n",
    "            # n_feature_maps[load_indices] += 1\n",
    "            feature_maps_sum[load_indices] = feature_maps_sum[load_indices] + feature_maps_\n",
    "            n_feature_maps[load_indices] =  n_feature_maps[load_indices] + 1\n",
    "\n",
    "    feature_maps_avg = feature_maps_sum / n_feature_maps[:, None, None, None]\n",
    "    feature_maps_avg = feature_maps_avg.detach().cpu().numpy()\n",
    "    all_features = feature_maps_avg[all_masks > 0] # K,128\n",
    "    os.makedirs(f\"/root/autodl-tmp/facescape/all_features/{idx}\", exist_ok=True)\n",
    "    np.save(f\"/root/autodl-tmp/facescape/all_features/{idx}/all_features.npy\", all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 7144.84it/s]\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "for idx in trange(1,101):\n",
    "    idx = str(idx)\n",
    "    all_points_raw_file = f\"/root/autodl-tmp/facescape/all_features_no_frontal/{idx}/all_points.npy\"\n",
    "    os.makedirs(f\"/root/autodl-tmp/facescape/all_features/{idx}\", exist_ok=True)\n",
    "    all_points_file = f\"/root/autodl-tmp/facescape/all_features/{idx}/all_points.npy\"\n",
    "    shutil.copy(all_points_raw_file, all_points_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 固定frontal view，加入layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frontal_view_img_idx_dict = {\n",
    "    \"2\":14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13777/1616818035.py:102: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(dtype=dtype):\n",
      "100%|██████████| 1/1 [03:02<00:00, 182.63s/it]\n"
     ]
    }
   ],
   "source": [
    "layernorm_fn = torch.nn.LayerNorm(128).to(device)\n",
    "\n",
    "for idx in tqdm(frontal_view_img_idx_dict.keys()):\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    frontal_view_img_idx = frontal_view_img_idx_dict[idx]\n",
    "    base_path = \"/root/autodl-tmp/facescape\"\n",
    "    img_path = os.path.join(base_path, \"mv_image\", idx)\n",
    "    depth_path = os.path.join(base_path, \"depth\", idx)\n",
    "    mask_path = os.path.join(base_path, \"mask\", idx)\n",
    "    params_path = os.path.join(base_path, \"params\", idx)\n",
    "\n",
    "    n_imgs = len(os.listdir(img_path))\n",
    "    def load_data(load_indices):\n",
    "        depth_map = []\n",
    "        extrinsic = []\n",
    "        intrinsic = []\n",
    "        images = []\n",
    "        masks = []\n",
    "\n",
    "        for load_index in load_indices:\n",
    "            if False:\n",
    "                img, depth, mask, extr, intr = data_cache[load_index]\n",
    "            else:\n",
    "                img = cv2.imread(os.path.join(img_path, f\"{load_index}.png\"))\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                # img = cv2.resize(img, (518, round(2592/1728*518)))\n",
    "                # img = img[130:130+518, 0:518]\n",
    "                depth = np.load(os.path.join(depth_path, f\"{load_index}.npy\"))\n",
    "                # depth = cv2.resize(depth, (518, round(2592/1728*518)))\n",
    "                # depth = depth[130:130+518, 0:518]\n",
    "                mask = np.load(os.path.join(mask_path, f\"{load_index}.npy\"))\n",
    "                # mask = cv2.resize(mask, (518, round(2592/1728*518)))\n",
    "                # mask = mask[130:130+518, 0:518]\n",
    "                params = np.load(os.path.join(params_path, f\"{load_index}.npz\"))\n",
    "                extr = params[\"extr\"]\n",
    "                intr = params[\"intr\"]\n",
    "                # intr = intr * 518 / 1728\n",
    "                # intr[1,2] = intr[1,2] - 130\n",
    "                # data_cache[load_index] = (img, depth, mask, extr, intr)\n",
    "            depth_map.append(depth)\n",
    "            extrinsic.append(extr)\n",
    "            intrinsic.append(intr)\n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "\n",
    "        images = np.array(images)\n",
    "        images = images / 255.\n",
    "        masks = np.array(masks)\n",
    "        depth_map = np.array(depth_map)\n",
    "        extrinsic = np.array(extrinsic)\n",
    "        intrinsic = np.array(intrinsic)\n",
    "        return images, depth_map, extrinsic, intrinsic, masks\n",
    "\n",
    "    all_images, all_depth_maps, all_extrinsics, all_intrinsics, all_masks = load_data(list(range(n_imgs)))\n",
    "\n",
    "    def load_data(load_indices):\n",
    "        load_indices = list(load_indices)\n",
    "        images = all_images[load_indices]\n",
    "        depth_maps = all_depth_maps[load_indices]\n",
    "        extrinsics = all_extrinsics[load_indices]\n",
    "        intrinsics = all_intrinsics[load_indices]\n",
    "        masks = all_masks[load_indices]\n",
    "        return images, depth_maps, extrinsics, intrinsics, masks\n",
    "\n",
    "    load_indices_samples = set()\n",
    "    while True:\n",
    "        samples = list(range(n_imgs))\n",
    "        random.shuffle(samples)\n",
    "        samples = samples[:16]\n",
    "        samples = sorted(samples)\n",
    "        if frontal_view_img_idx in samples:\n",
    "            # make sure the frontal view is the first image\n",
    "            samples.remove(frontal_view_img_idx)\n",
    "            samples.insert(0, frontal_view_img_idx)\n",
    "        else:\n",
    "            continue\n",
    "        load_indices_samples.add(tuple(samples))\n",
    "        if len(load_indices_samples) == 1000:\n",
    "            break\n",
    "    load_indices_samples = list(load_indices_samples)\n",
    "    train_indices = load_indices_samples[:800]\n",
    "    val_indices = load_indices_samples[800:900]\n",
    "    test_indices = load_indices_samples[900:]\n",
    "\n",
    "    feature_maps_sum = np.zeros((n_imgs, 518, 518, 128), dtype=np.float32)\n",
    "    n_feature_maps = np.zeros((n_imgs), dtype=np.float32)\n",
    "    feature_maps_sum = torch.zeros((n_imgs, 518, 518, 128), dtype=torch.float32).to(device)\n",
    "    n_feature_maps = torch.zeros((n_imgs), dtype=torch.float32).to(device)\n",
    "\n",
    "    for i in range(100):\n",
    "        load_indices = train_indices[i]\n",
    "        load_indices = list(load_indices)\n",
    "        images, depth_map, extrinsic, intrinsic, masks = load_data(load_indices)\n",
    "        images = torch.from_numpy(images).to(device).float()\n",
    "        # depth_map = torch.from_numpy(depth_map).to(device).unsqueeze(-1)\n",
    "        # extrinsic = torch.from_numpy(extrinsic).to(device)\n",
    "        # intrinsic = torch.from_numpy(intrinsic).to(device)\n",
    "        images = images.permute(0, 3, 1, 2) # 16, 518, 518, 3 -> 16, 3, 518, 518\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast(dtype=dtype):\n",
    "                images = images[None]  # add batch dimension\n",
    "                aggregated_tokens_list, ps_idx = model.aggregator(images)\n",
    "            feature_maps = model.track_head.feature_extractor(aggregated_tokens_list, images, ps_idx)[0]\n",
    "            del images\n",
    "            del aggregated_tokens_list\n",
    "            feature_maps = torch.nn.functional.interpolate(feature_maps, size=(518, 518), mode='bilinear', align_corners=False)\n",
    "            feature_maps = feature_maps.permute(0, 2, 3, 1) # 16, 518, 518, 128\n",
    "            feature_maps_ = feature_maps\n",
    "            fmaps = feature_maps_\n",
    "            \n",
    "            fmaps = layernorm_fn(fmaps.unsqueeze(0)).squeeze(0)  # apply layer normalization\n",
    "            feature_maps_ = fmaps\n",
    "            # feature_maps_ = feature_maps_.detach().cpu().numpy()  # convert to numpy array for further processing\n",
    "\n",
    "            # for j in range(len(load_indices)):\n",
    "            #     index = load_indices[j]\n",
    "            #     feature_maps_avg[index] = feature_maps_avg[index] * n_feature_maps[index] / (n_feature_maps[index]+1) + feature_maps_[j]/ (n_feature_maps[index]+1)\n",
    "            #     n_feature_maps[index] += 1\n",
    "            # feature_maps_avg[load_indices] = feature_maps_avg[load_indices] * n_feature_maps[load_indices][:, None, None, None] / (n_feature_maps[load_indices][:, None, None, None]+1) + feature_maps_ / (n_feature_maps[load_indices][:, None, None, None]+1)\n",
    "            # n_feature_maps[load_indices] += 1\n",
    "            feature_maps_sum[load_indices] = feature_maps_sum[load_indices] + feature_maps_\n",
    "            n_feature_maps[load_indices] =  n_feature_maps[load_indices] + 1\n",
    "\n",
    "    feature_maps_avg = feature_maps_sum / n_feature_maps[:, None, None, None]\n",
    "    feature_maps_avg = feature_maps_avg.detach().cpu().numpy()\n",
    "    all_features = feature_maps_avg[all_masks > 0] # K,128\n",
    "    os.makedirs(f\"/root/autodl-tmp/facescape/all_features_ln/{idx}\", exist_ok=True)\n",
    "    np.save(f\"/root/autodl-tmp/facescape/all_features_ln/{idx}/all_features.npy\", all_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dust3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
